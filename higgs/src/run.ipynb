{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from proj1_helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y, tx, ids = load_csv_data('../data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verified that the data is divided by categories indicated by column PRI_jet_num, which will be helpful to perform the preprocessing and data cleaning to deal the NaN values in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that there is a feature `PRI_jet_num` that categorizes the data. The Nan values vary according to its value. There are mainly 4 possibilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#22 is the column that indicates the category\n",
    "cat_index = 22\n",
    "\n",
    "cat0_row_indices = np.where(tx[:,cat_index] == 0)[0] \n",
    "cat1_row_indices = np.where(tx[:,cat_index] == 1)[0] \n",
    "cat2_row_indices = np.where(tx[:,cat_index] == 2)[0] \n",
    "cat3_row_indices = np.where(tx[:,cat_index] == 3)[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse the columns with NaN values for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cat0_NaNcol: [ 0  4  5  6 12 23 24 25 26 27 28] \n",
      " cat1_NaNcol: [ 0  4  5  6 12 26 27 28] \n",
      " cat2_NaNcol: [0] \n",
      " cat3_NaNcol: [0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat0_NaNcol = np.unique(np.where(tx[np.where(tx[:, cat_index] == 0)[0], :] == -999)[1])\n",
    "cat1_NaNcol = np.unique(np.where(tx[np.where(tx[:, cat_index] == 1)[0], :] == -999)[1])\n",
    "cat2_NaNcol = np.unique(np.where(tx[np.where(tx[:, cat_index] == 2)[0], :] == -999)[1])\n",
    "cat3_NaNcol = np.unique(np.where(tx[np.where(tx[:, cat_index] == 3)[0], :] == -999)[1])\n",
    "    \n",
    "#we see the columns for which there are NaN for each category.\n",
    "print(\" cat0_NaNcol:\", cat0_NaNcol,\"\\n\", \"cat1_NaNcol:\", cat1_NaNcol, \"\\n\", \"cat2_NaNcol:\", cat2_NaNcol, \n",
    "      \"\\n\", \"cat3_NaNcol:\", cat3_NaNcol, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23, 24, 25}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(cat0_NaNcol) - set(cat1_NaNcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat0_NaNcol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to check the `percentage of NaN values` of each column per category, in order to decide how to pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Category 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List output format: (column i has NaN value, percentage of NaN in column i)\n",
      "(0, 26.145746799715752)\n",
      "(4, 100.0)\n",
      "(5, 100.0)\n",
      "(6, 100.0)\n",
      "(12, 100.0)\n",
      "(23, 100.0)\n",
      "(24, 100.0)\n",
      "(25, 100.0)\n",
      "(26, 100.0)\n",
      "(27, 100.0)\n",
      "(28, 100.0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_NaN = np.asarray([len(np.where(tx[cat0_row_indices, i] == -999)[0]) \\\n",
    "                             /len(cat0_row_indices) for i in cat0_NaNcol]) * 100\n",
    "list_ = list(zip(cat0_NaNcol, percentage_NaN))  #(column i (has -999), percentage of -999 in that column)\n",
    "\n",
    "print(\"List output format: (column i has NaN value, percentage of NaN in column i)\")\n",
    "[print(i) for i in list_]\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Category 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List output format: (column i has NaN value, percentage of NaN in column i)\n",
      "(0, 9.7518828020220774)\n",
      "(4, 100.0)\n",
      "(5, 100.0)\n",
      "(6, 100.0)\n",
      "(12, 100.0)\n",
      "(26, 100.0)\n",
      "(27, 100.0)\n",
      "(28, 100.0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_NaN = np.asarray([len(np.where(tx[cat1_row_indices , i] == -999)[0]) \\\n",
    "                             /len(cat1_row_indices) for i in cat1_NaNcol]) * 100\n",
    "list_ = list(zip(cat1_NaNcol, percentage_NaN))  #(column i (has -999), percentage of -999 in that column)\n",
    "\n",
    "print(\"List output format: (column i has NaN value, percentage of NaN in column i)\")\n",
    "[print(i) for i in list_]\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Category 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List output format: (column i has NaN value, percentage of NaN in column i)\n",
      "(0, 5.8595843506222831)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_NaN = np.asarray([len(np.where(tx[cat2_row_indices , i] == -999)[0]) \\\n",
    "                             /len(cat2_row_indices) for i in cat2_NaNcol]) * 100\n",
    "list_ = list(zip(cat2_NaNcol, percentage_NaN))  #(column i (has -999), percentage of -999 in that column)\n",
    "\n",
    "print(\"List output format: (column i has NaN value, percentage of NaN in column i)\")\n",
    "[print(i) for i in list_]\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Category 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List output format: (column i has NaN value, percentage of NaN in column i)\n",
      "(0, 6.6639595740841013)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "percentage_NaN = np.asarray([len(np.where(tx[cat3_row_indices , i] == -999)[0]) \\\n",
    "                             /len(cat3_row_indices) for i in cat3_NaNcol]) * 100\n",
    "list_ = list(zip(cat3_NaNcol, percentage_NaN))  #(column i (has -999), percentage of -999 in that column)\n",
    "\n",
    "print(\"List output format: (column i has NaN value, percentage of NaN in column i)\")\n",
    "[print(i) for i in list_]\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that for each category, all the columns with NaN values, are 100% filled with NaN values, except for the first column, which  has NaN values distributed by all categories. furthermore, the total percentage of NaN values for the first column is low, therefore, we decided to replace the NaN values in this column by the average of the cleaned lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_lines_index = np.where(tx[:, 0] != -999)[0]\n",
    "avg = np.mean(tx[cleaned_lines_index, 0])\n",
    "NaN_lines_index = np.where(tx[:, 0] == -999)[0]\n",
    "tx[NaN_lines_index, 0] = avg*np.ones(len(NaN_lines_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, after this procedure, we will also remove this column from the NaN column list for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat0_NaNcol = cat0_NaNcol[1:]\n",
    "cat1_NaNcol = cat1_NaNcol[1:]\n",
    "cat2_NaNcol = cat2_NaNcol[1:]\n",
    "cat3_NaNcol = cat3_NaNcol[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('catNaNcol.pickle', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([cat0_NaNcol, cat1_NaNcol, cat2_NaNcol, cat3_NaNcol], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Creation by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we separate the data into categories and clean -999 columns except first column (only 15% NaN values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_cat0 = tx[cat0_row_indices, :]\n",
    "y_cat0  = y[cat0_row_indices]\n",
    "\n",
    "tx_cat1 = tx[cat1_row_indices, :]\n",
    "y_cat1  = y[cat1_row_indices]\n",
    "\n",
    "tx_cat2 = tx[cat2_row_indices, :]\n",
    "y_cat2  = y[cat2_row_indices]\n",
    "\n",
    "tx_cat3 = tx[cat3_row_indices, :]\n",
    "y_cat3  = y[cat3_row_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also concluded from the previous section that category 2 and 3 are basically the same. From now one category 2 = category 2 + category 3 (by merging them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_cat2 = np.vstack((tx_cat2, tx_cat3))\n",
    "y_cat2 = np.hstack((y_cat2, y_cat3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will also add the category column to the NaNcol set so that it is deleted next (not relevant)\n",
    "We verify that for category 0 the last column is also 0 always, so we eliminate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat0_toDelete = np.hstack((cat0_NaNcol, cat_index, np.array([29])))\n",
    "cat1_toDelete = np.hstack((cat1_NaNcol, cat_index))\n",
    "cat2_toDelete = np.array([cat_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete NaN columns for each tx set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tx_cat0 = np.delete(tx_cat0, cat0_toDelete, 1)\n",
    "tx_cat1 = np.delete(tx_cat1, cat1_toDelete, 1)\n",
    "tx_cat2 = np.delete(tx_cat2, cat2_toDelete, 1)\n",
    "\n",
    "y_cat = [y_cat0, y_cat1, y_cat2]\n",
    "tx_cat = [tx_cat0, tx_cat1, tx_cat2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information between output and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we calculate mutual information between input features and output in order to evaluate importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate mutual information between y and each feature\n",
    "\n",
    "def shan_entropy(c):\n",
    "    c_normalized = c / float(np.sum(c))\n",
    "    c_normalized = c_normalized[np.nonzero(c_normalized)]\n",
    "    H = -sum(c_normalized* np.log2(c_normalized))  \n",
    "    return H\n",
    "\n",
    "def calc_MI(X,Y, bins, bins_Y):\n",
    "    c_XY = np.histogram2d(X, Y, bins)[0]\n",
    "    c_X = np.histogram(X, bins)[0]\n",
    "    c_Y = np.histogram(Y, bins_Y)[0]\n",
    "\n",
    "    H_X = shan_entropy(c_X)\n",
    "    H_Y = shan_entropy(c_Y)\n",
    "    H_XY = shan_entropy(c_XY)\n",
    "\n",
    "    MI = H_X + H_Y - H_XY\n",
    "    return MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_features = []\n",
    "other_features = []\n",
    "max_features = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that calculates mutual information between output and each feature for all sets. \n",
    "It returns two lists, one with most important features and the other one with the remaining ones for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List output format: (column i , Mutual information with label) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"List output format: (column i , Mutual information with label)\", \"\\n\")\n",
    "def mutual_information():\n",
    "    bins = 50\n",
    "    \n",
    "    for i in range(len(tx_cat)):\n",
    "    \n",
    "        # Standardize Data, for iterative methods\n",
    "        tx_std, mean_x, std_x = standardize(tx_cat[i])\n",
    "        y = y_cat[i]\n",
    "\n",
    "        vecMI = np.zeros(np.shape(tx_std)[1])\n",
    "\n",
    "        for j in range(np.shape(tx_std)[1]):\n",
    "            vecMI[j] = calc_MI(tx_std[:, j], y, bins, 2)\n",
    "\n",
    "        top_features.append(np.argsort(-vecMI)[:max_features])\n",
    "        other_features.append(np.argsort(-vecMI)[max_features:])\n",
    "        \n",
    "        print('Top features for category {}: {}'.format(i,top_features[i]))\n",
    "        \n",
    "    return top_features, other_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features for category 0: [ 1  7  4  9  2  0  6  8 12]\n",
      "Top features for category 1: [ 0  2  1  8  9  7  4  6 19]\n",
      "Top features for category 2: [ 0  2  4  6  5 12  1 11 26]\n"
     ]
    }
   ],
   "source": [
    "top_features, other_features = mutual_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the objects:\n",
    "with open('top_features.pickle', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([top_features, other_features], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the first feature has a high relevance. On the other hand, columns 4, 12, 26, 27, 28 don't have as much importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's detect the possible outliers in out training data, and saved their row indices for posterior cleaning. \n",
    "We consider as outliers those values who vary from the mean value for more than 3 times the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_outliers(tx, mean, std):\n",
    "    outlier_indices = np.array([]) \n",
    "    for feature in range(tx.shape[1]):\n",
    "        row_indices = np.where(np.absolute(tx[:,feature]-mean[feature]) > 3*std[feature])[0]\n",
    "        mask = np.in1d(row_indices, outlier_indices)\n",
    "        outlier_indices = np.hstack((outlier_indices, row_indices[np.where(~mask)[0]]))\n",
    "\n",
    "        return outlier_indices.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete outliers for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of outliers for category 0 :\n",
      "1.8826378949686227\n",
      "percentage of outliers for category 1 :\n",
      "1.9098834210254823\n",
      "percentage of outliers for category 2 :\n",
      "2.06498214851881\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tx_cat)):\n",
    "    \n",
    "    # Standardize Data, for iterative methods\n",
    "    _, mean_x, std_x = standardize(tx_cat[i])\n",
    "    y = y_cat[i]   \n",
    "\n",
    "    print(\"percentage of outliers for category\", i, \":\")\n",
    "    print(len(detect_outliers(tx_cat[i], mean_x, std_x))/ tx_cat[i].shape[0] * 100)\n",
    "\n",
    "    outlier_indices = detect_outliers(tx_cat[i], mean_x, std_x)\n",
    "    tx_cat[i] = np.delete(tx_cat[i], outlier_indices, 0)\n",
    "    y_cat[i] = np.delete(y_cat[i], outlier_indices, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicitions on Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, as described in the report, we constructed our method on top of Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the parameter \"1 - accuracy\" as our loss function. This function calculates the ratio of the mispredicted y size and total number of labels. [see in implementations.py]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize Data\n",
    "for i in range(len(tx_cat)):\n",
    "    tx_cat[i], _, _ = standardize(tx_cat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f5d9d223ae8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#create a vector of lambdas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#define parameter for cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#create a vector of degrees\n",
    "degrees = [4, 5, 5]\n",
    "\n",
    "#create a vector of lambdas\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "\n",
    "#define parameter for cross-validation\n",
    "k_fold = 4\n",
    "\n",
    "#define seed\n",
    "seed = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying ridge regression:**\n",
    "- *A brief explanation:*\n",
    "This function receives as parameters the data from training.csv, a vector of degreees, a vector of lambdas, a cross validation parameter - k_fold and a seed.\n",
    "The aim of this routine is to iterate over several possibilities for the degree of the predictive function, as well as the lambda weight for the regularization term. \n",
    "For each pair (degree, function), the cross validation is computed, and its parameters are then averaged, and stored in two variables (loss_matrix to store losses, and w list).\n",
    "Finally, we find which parameters lead to the minimium in the matrix loss, and we follow on with those (correspondent w and degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_ridge_regression(y, tx, degrees, lambdas, k_fold, seed, top_features, other_features):\n",
    "    \n",
    "    #create varibles to store parameters\n",
    "    loss_matrix = np.zeros((len(degrees), len(lambdas))) \n",
    "    w_matrix = []\n",
    "    \n",
    "    #build vector of indices to use in cross validation\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "    #iterave over degrees and lambdas to find best pair of values:\n",
    "    for degree_index, degree in enumerate(degrees):    \n",
    "        #create multinomial for degree:\n",
    "        multinomial_i = build_multinomial(tx, degree, top_features, other_features)     \n",
    "        #list of weights (with length lambda) for degree_index:\n",
    "        w_lambda = []\n",
    "        \n",
    "        for lambda_index, lambda_ in enumerate(lambdas):        \n",
    "            w_cv = []\n",
    "            loss_cv = []\n",
    "            \n",
    "            #perform cross validation\n",
    "            for k in range(k_fold):\n",
    "                w_cv_k, loss_cv_k = cross_validation(y, tx, k_indices, k, lambda_, degree, multinomial_i)\n",
    "                w_cv.append(w_cv_k) \n",
    "                loss_cv.append(loss_cv_k)\n",
    "                \n",
    "            #average parameters\n",
    "            w_cv_avg = (np.asarray(w_cv).T).mean(axis=1)\n",
    "            loss_te_avg = np.mean(loss_cv)\n",
    "            \n",
    "            #store parameters\n",
    "            w_lambda.append(w_cv_avg)\n",
    "            loss_matrix[degree_index, lambda_index] = loss_te_avg\n",
    "            \n",
    "        w_matrix.append(w_lambda)\n",
    "        print(\"degree: \", degree)\n",
    "    \n",
    "    #find minimium \n",
    "    min_index = np.argwhere(loss_matrix == np.min(loss_matrix))\n",
    "    print(\"Ridge Regression loss* =\",np.min(loss_matrix))\n",
    "    print(degrees[min_index[0,0]], lambdas[min_index[0,1]])\n",
    "    \n",
    "    return w_matrix[min_index[0,0]][min_index[0,1]], degrees[min_index[0,0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree:  4\n",
      "Ridge Regression loss* = 0.153052064632\n",
      "4 4.21696503429e-05\n",
      "degree:  5\n",
      "Ridge Regression loss* = 0.189389955298\n",
      "5 0.00177827941004\n",
      "degree:  5\n",
      "Ridge Regression loss* = 0.166263160858\n",
      "5 0.000316227766017\n"
     ]
    }
   ],
   "source": [
    "w_rg = []\n",
    "degree_rg = []\n",
    "\n",
    "for i in range(len(tx_cat)):\n",
    "    \n",
    "    w_rg_i, degree_rg_i = apply_ridge_regression(y_cat[i], tx_cat[i], [degrees[i]], lambdas, k_fold, seed,\n",
    "                                                 top_features[i], other_features[i])\n",
    "    w_rg.append(w_rg_i)\n",
    "    degree_rg.append(degree_rg_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the objects:\n",
    "with open('objs.pickle', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([w_rg, degree_rg], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Data from test.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting back the objects:\n",
    "with open('objs.pickle', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    w_rg, degree_rg = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('top_features.pickle', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    top_features, other_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('catNaNcol.pickle', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    cat0_NaNcol, cat1_NaNcol, cat2_NaNcol, cat3_NaNcol = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568238\n"
     ]
    }
   ],
   "source": [
    "_, tx_predict, ids_predict = load_csv_data('../data/test.csv', sub_sample=False)\n",
    "print(tx_predict.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to what was done previoulsy, we analyse test data and check it has the same format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Category analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From test data, define row indices for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#22 is the column that indicates the category\n",
    "cat_index = 22\n",
    "\n",
    "cat0_row_indices = np.where(tx_predict[:,cat_index] == 0)[0] \n",
    "cat1_row_indices = np.where(tx_predict[:,cat_index] == 1)[0] \n",
    "cat2_row_indices = np.where(tx_predict[:,cat_index] == 2)[0] \n",
    "cat3_row_indices = np.where(tx_predict[:,cat_index] == 3)[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verified, on background, that the type of NaN values distribution is fairly the same as for the training data.\n",
    "So, similarly, we \"clean\" the NaN values of the first column using the mean of the cleaned lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get cleaned lines and average them\n",
    "cleaned_lines_index = np.where(tx_predict[:, 0] != -999)[0]\n",
    "avg = np.mean(tx_predict[cleaned_lines_index, 0])\n",
    "\n",
    "#replace with average value calculated above\n",
    "NaN_lines_index = np.where(tx_predict[:, 0] == -999)[0]\n",
    "tx_predict[NaN_lines_index, 0] = avg*np.ones(len(NaN_lines_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Creation by category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we separate the data into categories and clean -999 columns except first column (only 15% NaN values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_cat0 = tx_predict[cat0_row_indices, :]\n",
    "tx_cat1 = tx_predict[cat1_row_indices, :]\n",
    "tx_cat2 = tx_predict[cat2_row_indices, :]\n",
    "tx_cat3 = tx_predict[cat3_row_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done before, we merge category 2 and 3 due to their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_cat2 = tx_predict[np.where(tx_predict[:, cat_index] >= 2)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define columns to delete**\n",
    "\n",
    "Now, we will also add the category column to the NaNcol set so that it is deleted next (not relevant)\n",
    "We verify that for category 0 the last column is also 0 always, so we eliminate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat0_toDelete = np.hstack((cat0_NaNcol, cat_index, np.array([29])))\n",
    "cat1_toDelete = np.hstack((cat1_NaNcol, cat_index))\n",
    "cat2_toDelete = np.array([cat_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete NaN columns for each tx set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tx_cat0 = np.delete(tx_cat0, cat0_toDelete, 1)\n",
    "tx_cat1 = np.delete(tx_cat1, cat1_toDelete, 1)\n",
    "tx_cat2 = np.delete(tx_cat2, cat2_toDelete, 1)\n",
    "\n",
    "tx_cat = [tx_cat0, tx_cat1, tx_cat2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize Data\n",
    "for i in range(len(tx_cat)):\n",
    "    tx_cat[i], _, _ = standardize(tx_cat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Expansion for each Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "Phi predict (175338, 1357)\n"
     ]
    }
   ],
   "source": [
    "#According to ridge regression, build multinomial for degree corresponding to minimal loss\n",
    "phi_predict = []\n",
    "for i in range(len(tx_cat)):\n",
    "    phi_predict.append(build_multinomial(tx_cat[i], degree_rg[i], top_features[i], other_features[i]))\n",
    "    print(i)\n",
    "print('Phi predict', np.shape(phi_predict[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict = np.zeros(tx_predict.shape[0])\n",
    "\n",
    "category_col = tx_predict[:,cat_index]\n",
    "#Merge categories 2 and 3 into one single category (2)\n",
    "cat3_idx = np.where(category_col == 3)\n",
    "category_col[cat3_idx] = 2*np.ones(len(cat3_idx))\n",
    "\n",
    "\n",
    "for i in range(len(tx_cat)):\n",
    "    y_predict[np.where(category_col == i)[0]] = predict_labels(w_rg[i], phi_predict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids_predict, y_predict, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
